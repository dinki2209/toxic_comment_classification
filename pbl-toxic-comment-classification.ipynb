{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12500,"databundleVersionId":1375107,"sourceType":"competition"},{"sourceId":6881454,"sourceType":"datasetVersion","datasetId":3953726},{"sourceId":6882037,"sourceType":"datasetVersion","datasetId":3954050}],"dockerImageVersionId":30097,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom seaborn import kdeplot\nfrom bs4 import BeautifulSoup\nimport re, string \nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer \nfrom nltk.stem import WordNetLemmatizer \nimport unicodedata\nimport html\nimport numpy as np\nimport nltk","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-23T05:14:32.197168Z","iopub.execute_input":"2023-11-23T05:14:32.197506Z","iopub.status.idle":"2023-11-23T05:14:33.063080Z","shell.execute_reply.started":"2023-11-23T05:14:32.197475Z","shell.execute_reply":"2023-11-23T05:14:33.062329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install BeautifulSoup4","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:14:18.776831Z","iopub.execute_input":"2023-11-23T05:14:18.777284Z","iopub.status.idle":"2023-11-23T05:14:27.319478Z","shell.execute_reply.started":"2023-11-23T05:14:18.777231Z","shell.execute_reply":"2023-11-23T05:14:27.318443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\")\ntest = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:14:36.840385Z","iopub.execute_input":"2023-11-23T05:14:36.840751Z","iopub.status.idle":"2023-11-23T05:14:57.712865Z","shell.execute_reply.started":"2023-11-23T05:14:36.840718Z","shell.execute_reply":"2023-11-23T05:14:57.711993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:14:59.352461Z","iopub.execute_input":"2023-11-23T05:14:59.352849Z","iopub.status.idle":"2023-11-23T05:14:59.376166Z","shell.execute_reply.started":"2023-11-23T05:14:59.352816Z","shell.execute_reply":"2023-11-23T05:14:59.375237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:15:00.550717Z","iopub.execute_input":"2023-11-23T05:15:00.551072Z","iopub.status.idle":"2023-11-23T05:15:00.582962Z","shell.execute_reply.started":"2023-11-23T05:15:00.551039Z","shell.execute_reply":"2023-11-23T05:15:00.581953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sample = train.sample(frac = 0.1, random_state = 42, axis = 'index')","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:15:01.796213Z","iopub.execute_input":"2023-11-23T05:15:01.796552Z","iopub.status.idle":"2023-11-23T05:15:02.016619Z","shell.execute_reply.started":"2023-11-23T05:15:01.796523Z","shell.execute_reply":"2023-11-23T05:15:02.015872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text Cleaning","metadata":{}},{"cell_type":"code","source":"from bs4 import BeautifulSoup # Text Cleaning\nimport re, string # Regular Expressions, String\nfrom nltk.corpus import stopwords # stopwords\nfrom nltk.stem.porter import PorterStemmer # for word stemming\nfrom nltk.stem import WordNetLemmatizer # for word lemmatization\nimport unicodedata\nimport html\n\n# set of stopwords to be removed from text\nstop = set(stopwords.words('english'))\n\n# update stopwords to have punctuation too\nstop.update(list(string.punctuation))\n\ndef clean_text(text):\n    # Remove unwanted html characters\n    re1 = re.compile(r'  +')\n    x1 = text.lower().replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n    'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n    '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n    ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n    text = re1.sub(' ', html.unescape(x1))\n    # remove non-ascii characters\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    \n#     # strip html\n#     soup = BeautifulSoup(text, 'html.parser')\n#     text = soup.get_text()\n    \n    # remove between square brackets\n    text = re.sub('\\[[^]]*\\]', '', text)\n    \n    # remove URLs\n    text = re.sub(r'http\\S+', '', text)\n    \n    # remove twitter tags\n    text = text.replace(\"@\", \"\")\n    \n    # remove hashtags\n    text = text.replace(\"#\", \"\")\n    \n    # remove all non-alphabetic characters\n    text = re.sub(r'[^a-zA-Z ]', '', text)\n    \n    # remove stopwords from text\n    final_text = []\n    for word in text.split():\n        if word.strip().lower() not in stop:\n            final_text.append(word.strip().lower())\n    \n    text = \" \".join(final_text)\n    \n    # lemmatize words\n    lemmatizer = WordNetLemmatizer()    \n    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n    text = \" \".join([lemmatizer.lemmatize(word, pos = 'v') for word in text.split()])\n    # replace all numbers with \"num\"\n    text = re.sub(\"\\d\", \"num\", text)\n    return text.lower()","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:15:03.811290Z","iopub.execute_input":"2023-11-23T05:15:03.811678Z","iopub.status.idle":"2023-11-23T05:15:03.830310Z","shell.execute_reply.started":"2023-11-23T05:15:03.811642Z","shell.execute_reply":"2023-11-23T05:15:03.829496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = train\ntrain_data['threat'] = np.where(train_data['threat'] >= .25, 1, 0)\ntrain_data['severe_toxicity'] = np.where(train_data['severe_toxicity'] >= .25, 1, 0)\ntrain_data['insult'] = np.where(train_data['insult'] >= .25, 1, 0)\ntrain_data['obscene'] = np.where(train_data['obscene'] >= .25, 1, 0)\ntrain_data['identity_attack'] = np.where(train_data['identity_attack'] >= .25, 1, 0)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:15:05.890245Z","iopub.execute_input":"2023-11-23T05:15:05.890616Z","iopub.status.idle":"2023-11-23T05:15:06.537952Z","shell.execute_reply.started":"2023-11-23T05:15:05.890561Z","shell.execute_reply":"2023-11-23T05:15:06.537138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"possible_labels = [\"comment_text\",\"target\",\"severe_toxicity\", \"obscene\", \"threat\", \"insult\", \"identity_attack\"]\n","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:15:07.620877Z","iopub.execute_input":"2023-11-23T05:15:07.621200Z","iopub.status.idle":"2023-11-23T05:15:07.625585Z","shell.execute_reply.started":"2023-11-23T05:15:07.621173Z","shell.execute_reply":"2023-11-23T05:15:07.624521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_final = train_data[possible_labels]\ntrain_data_final = train_data_final[(train_data_final['target']>=.5)]\ntrain_data_final.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:15:14.775210Z","iopub.execute_input":"2023-11-23T05:15:14.775554Z","iopub.status.idle":"2023-11-23T05:15:15.025893Z","shell.execute_reply.started":"2023-11-23T05:15:14.775525Z","shell.execute_reply":"2023-11-23T05:15:15.024978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"possible_labels1 = [\"severe_toxicity\", \"obscene\", \"threat\", \"insult\", \"identity_attack\"]\ntargets = train_data_final[possible_labels].values","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:15:16.275877Z","iopub.execute_input":"2023-11-23T05:15:16.276201Z","iopub.status.idle":"2023-11-23T05:15:16.320777Z","shell.execute_reply.started":"2023-11-23T05:15:16.276175Z","shell.execute_reply":"2023-11-23T05:15:16.319828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_data_final)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:15:17.410812Z","iopub.execute_input":"2023-11-23T05:15:17.411163Z","iopub.status.idle":"2023-11-23T05:15:17.416407Z","shell.execute_reply.started":"2023-11-23T05:15:17.411134Z","shell.execute_reply":"2023-11-23T05:15:17.415421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_final['clean_comment_text'] = train_data_final['comment_text'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:15:18.622015Z","iopub.execute_input":"2023-11-23T05:15:18.622344Z","iopub.status.idle":"2023-11-23T05:16:11.363349Z","shell.execute_reply.started":"2023-11-23T05:15:18.622316Z","shell.execute_reply":"2023-11-23T05:16:11.362476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_final.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:16:16.098815Z","iopub.execute_input":"2023-11-23T05:16:16.099163Z","iopub.status.idle":"2023-11-23T05:16:16.113365Z","shell.execute_reply.started":"2023-11-23T05:16:16.099136Z","shell.execute_reply":"2023-11-23T05:16:16.112453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sequences creation, truncation and padding\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# Setting up the tokenizer\nvocab_size = 10000\ntokenizer = Tokenizer(num_words = vocab_size, oov_token = 'UNK')\ntokenizer.fit_on_texts(list(train_data_final['clean_comment_text']))\n\nmax_len = 18\nX_train_seq = tokenizer.texts_to_sequences(train_data_final['clean_comment_text'])\n\nX_train_seq = pad_sequences(X_train_seq, maxlen = max_len, truncating = 'post', padding = 'post')\n#X_test_seq = pad_sequences(X_test_seq, maxlen = max_len, truncating = 'post', padding = 'post')\n\n\nprint(f\"X_train shape: {X_train_seq.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:16:17.570018Z","iopub.execute_input":"2023-11-23T05:16:17.570391Z","iopub.status.idle":"2023-11-23T05:16:31.828976Z","shell.execute_reply.started":"2023-11-23T05:16:17.570358Z","shell.execute_reply":"2023-11-23T05:16:31.827873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = np.array(train_data_final[possible_labels1]).astype(int)\nprint(f\"y_train shape: {y_train.shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:16:34.077831Z","iopub.execute_input":"2023-11-23T05:16:34.078173Z","iopub.status.idle":"2023-11-23T05:16:34.101469Z","shell.execute_reply.started":"2023-11-23T05:16:34.078144Z","shell.execute_reply":"2023-11-23T05:16:34.100506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Validation Split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train_seq, X_val_seq, y_train, y_val = train_test_split(X_train_seq, y_train, test_size = 0.2, random_state = 42)\n\nprint(f\"X_train shape: {X_train_seq.shape}\")\nprint(f\"X_val shape: {X_val_seq.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"y_val shape: {y_val.shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:16:39.396923Z","iopub.execute_input":"2023-11-23T05:16:39.397276Z","iopub.status.idle":"2023-11-23T05:16:39.425849Z","shell.execute_reply.started":"2023-11-23T05:16:39.397246Z","shell.execute_reply":"2023-11-23T05:16:39.424843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_words = len(tokenizer.word_index)\nprint(f\"Number of unique words: {num_words}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:16:40.585745Z","iopub.execute_input":"2023-11-23T05:16:40.586092Z","iopub.status.idle":"2023-11-23T05:16:40.591010Z","shell.execute_reply.started":"2023-11-23T05:16:40.586058Z","shell.execute_reply":"2023-11-23T05:16:40.590146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef load_embeddings(embedding_file):\n    embedding_dict = {}\n    with open(embedding_file, 'r', encoding='utf-8') as file:\n        for line in file:\n            values = line.strip().split()\n            word = values[0]\n            vector = np.array(values[1:], dtype='float32')\n            embedding_dict[word] = vector\n    return embedding_dict\n\n# Specify the path to your GloVe file\nembedding_file = '/kaggle/input/addedd/glove.42B.300d.txt'\nembedding_dict = load_embeddings(embedding_file)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:16:41.781295Z","iopub.execute_input":"2023-11-23T05:16:41.781732Z","iopub.status.idle":"2023-11-23T05:20:32.659692Z","shell.execute_reply.started":"2023-11-23T05:16:41.781683Z","shell.execute_reply":"2023-11-23T05:20:32.658848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying GloVE representations on our corpus\n\nembedding_matrix=np.zeros((num_words,300))\n\nfor word,i in tokenizer.word_index.items():\n    if i < num_words:\n        emb_vec = embedding_dict.get(word)\n        if emb_vec is not None:\n            embedding_matrix[i] = emb_vec    \n            \nembedding_matrix.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:20:33.375240Z","iopub.execute_input":"2023-11-23T05:20:33.375575Z","iopub.status.idle":"2023-11-23T05:20:33.722195Z","shell.execute_reply.started":"2023-11-23T05:20:33.375545Z","shell.execute_reply":"2023-11-23T05:20:33.721149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LSTM","metadata":{}},{"cell_type":"code","source":"# Setting up the model\n\nfrom keras import layers\nfrom keras.models import Sequential\n\ndef setup_lstm_model(max_len, n_latent_factors):\n    \n    model = Sequential()\n    model.add(layers.Embedding(num_words, n_latent_factors, weights = [embedding_matrix], \n                               input_length = max_len, trainable = False))\n    model.add(layers.LSTM(units = max_len, return_sequences = True))\n    model.add(layers.GlobalAveragePooling1D())\n    model.add(layers.Dense(units = 5, activation = 'sigmoid'))\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:20:59.267103Z","iopub.execute_input":"2023-11-23T05:20:59.267458Z","iopub.status.idle":"2023-11-23T05:20:59.273801Z","shell.execute_reply.started":"2023-11-23T05:20:59.267428Z","shell.execute_reply":"2023-11-23T05:20:59.272895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lstm_model = setup_lstm_model(max_len = max_len, n_latent_factors = 300)\nlstm_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:21:02.042964Z","iopub.execute_input":"2023-11-23T05:21:02.043319Z","iopub.status.idle":"2023-11-23T05:21:05.033648Z","shell.execute_reply.started":"2023-11-23T05:21:02.043284Z","shell.execute_reply":"2023-11-23T05:21:05.032742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Final hyperparameter configurations\nlstm_model.compile(optimizer = 'adam', loss = 'binary_crossentropy',metrics=['accuracy'])\n\nbatch_size = 128\nepochs = 10\n\nlstm_model.fit(X_train_seq, y_train, epochs=epochs, batch_size = batch_size, validation_data = (X_val_seq, y_val))","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:21:09.127276Z","iopub.execute_input":"2023-11-23T05:21:09.127680Z","iopub.status.idle":"2023-11-23T05:21:58.172711Z","shell.execute_reply.started":"2023-11-23T05:21:09.127641Z","shell.execute_reply":"2023-11-23T05:21:58.171821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lstm_model.history.history","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:22:11.454212Z","iopub.execute_input":"2023-11-23T05:22:11.454550Z","iopub.status.idle":"2023-11-23T05:22:11.460697Z","shell.execute_reply.started":"2023-11-23T05:22:11.454521Z","shell.execute_reply":"2023-11-23T05:22:11.459769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ny_pred = lstm_model.predict(X_val_seq)\ny_pred_binary = (y_pred >= 0.5).astype(int)  # Convert probabilities to binary labels\naccuracy = accuracy_score(y_val, y_pred_binary)\nprint(accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:22:13.652200Z","iopub.execute_input":"2023-11-23T05:22:13.652565Z","iopub.status.idle":"2023-11-23T05:22:15.512874Z","shell.execute_reply.started":"2023-11-23T05:22:13.652530Z","shell.execute_reply":"2023-11-23T05:22:15.511891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score\n\n# Assuming you have binary matrices for true labels (y_val) and predicted labels (y_pred)\n\n# Calculate precision without specifying average\nprecision = precision_score(y_val, y_pred_binary, average=None)\n\n# Calculate recall without specifying average\nrecall = recall_score(y_val, y_pred_binary, average=None)\n\n# Print the results for each label\nfor label in range(y_val.shape[1]):\n    label_precision = precision[label]\n    label_recall = recall[label]\n    print(f'Label {label} - Precision: {label_precision:.4f}, Recall: {label_recall:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:22:17.214957Z","iopub.execute_input":"2023-11-23T05:22:17.215296Z","iopub.status.idle":"2023-11-23T05:22:17.272958Z","shell.execute_reply.started":"2023-11-23T05:22:17.215268Z","shell.execute_reply":"2023-11-23T05:22:17.272060Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score\n\n# Assuming you have binary matrices for true labels (y_val) and predicted labels (y_pred_binary)\n\n# Calculate F1-score without specifying average\nf1 = f1_score(y_val, y_pred_binary, average=None)\n\n# Print the results for each label\nfor label in range(y_val.shape[1]):\n    label_f1 = f1[label]\n    print(f'Label {label} - F1-Score: {label_f1:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:22:19.063442Z","iopub.execute_input":"2023-11-23T05:22:19.063841Z","iopub.status.idle":"2023-11-23T05:22:19.095136Z","shell.execute_reply.started":"2023-11-23T05:22:19.063806Z","shell.execute_reply":"2023-11-23T05:22:19.094248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testing=pd.read_csv(\"/kaggle/input/testdataset/testing.csv\")\nimport numpy as np\n\n# Initialize an empty list to store the predicted toxicity scores\npredicted_toxicity_scores = []\n\n# Loop through each comment in the test data\nfor comment in testing['comment_text']:\n    # Preprocess the comment\n    preprocessed_comment = clean_text(comment)  # Apply your text cleaning and preprocessing\n    \n    # Tokenize and convert the preprocessed comment into a sequence\n    comment_seq = tokenizer.texts_to_sequences([preprocessed_comment])\n    comment_seq = pad_sequences(comment_seq, maxlen=max_len)  # Make sure max_len matches your training data\n    \n    # Make a prediction for the comment\n    toxicity_score = lstm_model.predict(comment_seq)\n    \n    # Append the predicted toxicity score to the list\n    predicted_toxicity_scores.append(toxicity_score)\n\n# Convert the list of predicted toxicity scores to a NumPy array for further analysis\npredicted_toxicity_scores = np.array(predicted_toxicity_scores)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:22:45.092915Z","iopub.execute_input":"2023-11-23T05:22:45.093249Z","iopub.status.idle":"2023-11-23T05:22:46.856435Z","shell.execute_reply.started":"2023-11-23T05:22:45.093221Z","shell.execute_reply":"2023-11-23T05:22:46.855688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_samples, num_predictions, num_labels = predicted_toxicity_scores.shape\npredicted_scores_2d = predicted_toxicity_scores.reshape(num_samples, num_labels)\n\n# Create a DataFrame with columns for each label\ncolumn_names = [\"severe_toxicity\", \"obscene\", \"threat\", \"insult\", \"identity_attack\"]\npredicted_scores_df = pd.DataFrame(data=predicted_scores_2d, columns=column_names)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:22:49.344073Z","iopub.execute_input":"2023-11-23T05:22:49.344428Z","iopub.status.idle":"2023-11-23T05:22:49.349551Z","shell.execute_reply.started":"2023-11-23T05:22:49.344397Z","shell.execute_reply":"2023-11-23T05:22:49.348755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_scores_df","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:22:50.822833Z","iopub.execute_input":"2023-11-23T05:22:50.823215Z","iopub.status.idle":"2023-11-23T05:22:50.845806Z","shell.execute_reply.started":"2023-11-23T05:22:50.823182Z","shell.execute_reply":"2023-11-23T05:22:50.844810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}